p8105_hw2_cm3341
================
Carolina Montes Garcia
2024-10-01

- [Problem 0](#problem-0)
- [Problem 1](#problem-1)
- [Problem 2](#problem-2)
- [Problem 3](#problem-3)

### Problem 0

This report follows the required best practices for an rmd file with a
corresponding github repository. Commits were executed throughout the
entirety of the project. Due to time constrains, I used the shared code
for problem 1.

I being with:

Load libraries

``` r
library(tidyverse)
library(readxl)
library(knitr)
```

## Problem 1

Below we import and clean data from
`NYC_Transit_Subway_Entrance_And_Exit_Data.csv`. The process begins with
data import, updates variable names, and selects the columns that will
be used in later parts fo this problem. We update `entry` from `yes` /
`no` to a logical variable. As part of data import, we specify that
`Route` columns 8-11 should be character for consistency with 1-7.

``` r
trans_ent = 
  read_csv(
    "data/NYC_Transit_Subway_Entrance_And_Exit_Data.csv",
    col_types = cols(Route8 = "c", Route9 = "c", Route10 = "c", Route11 = "c")) |> 
  janitor::clean_names() |> 
  select(
    line, station_name, station_latitude, station_longitude, 
    starts_with("route"), entry, exit_only, vending, entrance_type, 
    ada) |> 
  mutate(entry = ifelse(entry == "YES", TRUE, FALSE))
```

As it stands, these data are not “tidy”: route number should be a
variable, as should route. That is, to obtain a tidy dataset we would
need to convert `route` variables from wide to long format. This will be
useful when focusing on specific routes, but may not be necessary when
considering questions that focus on station-level variables.

The following code chunk selects station name and line, and then uses
`distinct()` to obtain all unique combinations. As a result, the number
of rows in this dataset is the number of unique stations. .

``` r
trans_ent |> 
  select(station_name, line) |> 
  distinct()
```

    ## # A tibble: 465 × 2
    ##    station_name             line    
    ##    <chr>                    <chr>   
    ##  1 25th St                  4 Avenue
    ##  2 36th St                  4 Avenue
    ##  3 45th St                  4 Avenue
    ##  4 53rd St                  4 Avenue
    ##  5 59th St                  4 Avenue
    ##  6 77th St                  4 Avenue
    ##  7 86th St                  4 Avenue
    ##  8 95th St                  4 Avenue
    ##  9 9th St                   4 Avenue
    ## 10 Atlantic Av-Barclays Ctr 4 Avenue
    ## # ℹ 455 more rows

The next code chunk is similar, but filters according to ADA compliance
as an initial step. This produces a dataframe in which the number of
rows is the number of ADA compliant stations.

``` r
trans_ent |> 
  filter(ada == TRUE) |> 
  select(station_name, line) |> 
  distinct()
```

    ## # A tibble: 84 × 2
    ##    station_name                   line           
    ##    <chr>                          <chr>          
    ##  1 Atlantic Av-Barclays Ctr       4 Avenue       
    ##  2 DeKalb Av                      4 Avenue       
    ##  3 Pacific St                     4 Avenue       
    ##  4 Grand Central                  42nd St Shuttle
    ##  5 34th St                        6 Avenue       
    ##  6 47-50th Sts Rockefeller Center 6 Avenue       
    ##  7 Church Av                      6 Avenue       
    ##  8 21st St                        63rd Street    
    ##  9 Lexington Av                   63rd Street    
    ## 10 Roosevelt Island               63rd Street    
    ## # ℹ 74 more rows

To compute the proportion of station entrances / exits without vending
allow entrance, we first exclude station entrances that do not allow
vending. Then, we focus on the `entry` variable – this logical, so
taking the mean will produce the desired proportion (recall that R will
coerce logical to numeric in cases like this).

``` r
trans_ent |> 
  filter(vending == "NO") |> 
  pull(entry) |> 
  mean()
```

    ## [1] 0.3770492

Lastly, we write a code chunk to identify stations that serve the A
train, and to assess how many of these are ADA compliant. As a first
step, we tidy the data as alluded to previously; that is, we convert
`route` from wide to long format. After this step, we can use tools from
previous parts of the question (filtering to focus on the A train, and
on ADA compliance; selecting and using `distinct` to obtain dataframes
with the required stations in rows).

``` r
trans_ent |> 
  pivot_longer(
    route1:route11,
    names_to = "route_num",
    values_to = "route") |> 
  filter(route == "A") |> 
  select(station_name, line) |> 
  distinct()
```

    ## # A tibble: 60 × 2
    ##    station_name                  line           
    ##    <chr>                         <chr>          
    ##  1 Times Square                  42nd St Shuttle
    ##  2 125th St                      8 Avenue       
    ##  3 145th St                      8 Avenue       
    ##  4 14th St                       8 Avenue       
    ##  5 168th St - Washington Heights 8 Avenue       
    ##  6 175th St                      8 Avenue       
    ##  7 181st St                      8 Avenue       
    ##  8 190th St                      8 Avenue       
    ##  9 34th St                       8 Avenue       
    ## 10 42nd St                       8 Avenue       
    ## # ℹ 50 more rows

``` r
trans_ent |> 
  pivot_longer(
    route1:route11,
    names_to = "route_num",
    values_to = "route") |> 
  filter(route == "A", ada == TRUE) |> 
  select(station_name, line) |> 
  distinct()
```

    ## # A tibble: 17 × 2
    ##    station_name                  line            
    ##    <chr>                         <chr>           
    ##  1 14th St                       8 Avenue        
    ##  2 168th St - Washington Heights 8 Avenue        
    ##  3 175th St                      8 Avenue        
    ##  4 34th St                       8 Avenue        
    ##  5 42nd St                       8 Avenue        
    ##  6 59th St                       8 Avenue        
    ##  7 Inwood - 207th St             8 Avenue        
    ##  8 West 4th St                   8 Avenue        
    ##  9 World Trade Center            8 Avenue        
    ## 10 Times Square-42nd St          Broadway        
    ## 11 59th St-Columbus Circle       Broadway-7th Ave
    ## 12 Times Square                  Broadway-7th Ave
    ## 13 8th Av                        Canarsie        
    ## 14 Franklin Av                   Franklin        
    ## 15 Euclid Av                     Fulton          
    ## 16 Franklin Av                   Fulton          
    ## 17 Howard Beach                  Rockaway

## Problem 2

This problem uses trash wheel datasets for three different trash wheel
garbage collector vehicles (Mr. Trash Wheel, Professor Trash Wheel, and
Gwynnda Trash Wheel). These machines clean debris from various harbors.
First, I imported and cleaned the Mr. trash wheel dataset. The *year*
variable has to be mutated into the same type of variable so that the
binding function can work the dataset merging step later on. I chose to
make all years into numeric variables using `as.numeric`.

In my import, I skipped the first row as it contained unimportant
information. Additionally, I used `slice` to remove the last two rows of
the dataset that included some calculations. I then removed two more
random columns that appear after import called *x15* and *x16*, which
contained no information at all.

``` r
mr_trash_wheel_data = 
  read_excel(
    "data/202409 Trash Wheel Collection Data.xlsx", 
    sheet = "Mr. Trash Wheel", 
    skip = 1, 
    na = c("NA", ".")) %>%
  janitor::clean_names() %>%
  slice(
    1:(n() - 2)) %>%
  mutate(
    sports_balls = as.integer(round(sports_balls, 0)),  
         trash_wheel = "Mr. Trash Wheel")%>%
  select(
    -x15, -x16)%>%
  mutate(
    year = as.numeric(year))
```

Import and clean professor trash wheel data. In my import, I skipped the
first row as it contained unimportant information. Additionally, I used
`slice` to remove the last two rows of the dataset that included some
calculations.

``` r
prof_wheel_data = 
  read_excel(
    "data/202409 Trash Wheel Collection Data.xlsx", 
    sheet = "Professor Trash Wheel", 
    skip = 1, 
    na = c("NA", ".")) %>%
  janitor::clean_names() %>%
  slice(
    1:(n() - 2)) %>%
  mutate(
    trash_wheel = "Professor Trash Wheel")%>%
  mutate(
    year = as.numeric(year))
```

Import and clean gwynnda trash wheel data.In my import, I skipped the
first row as it contained unimportant information. Additionally, I used
`slice` to remove the last row of the dataset that included some
calculations.

``` r
gwynnda_wheel_data = 
  read_excel(
    "data/202409 Trash Wheel Collection Data.xlsx", 
    sheet = "Gwynnda Trash Wheel", 
    skip = 1, 
    na = c("NA", ".")) %>%
  janitor::clean_names() %>%
  slice(
    1:(n() - 1)) %>%
  mutate(
    trash_wheel = "Gwynnda Trash Wheel")%>%
  mutate(
    year = as.numeric(year))
```

I combined the three datasets using the `bind_rows` function, since I
need these rows to stack on top of each other. I tried merge functions
first, but that didn’t work.

``` r
full_trash_wheel_data = 
  bind_rows(
    mr_trash_wheel_data, 
    prof_wheel_data, 
    gwynnda_wheel_data)
```

Calculated \# of total observations in the combined dataset

``` r
total_rows = 
  nrow(
    full_trash_wheel_data)
```

Calculated how many tons of trash were collected by professor trash
wheel.

``` r
prof_trash_weight = 
  full_trash_wheel_data %>%
  filter(
    trash_wheel == "Professor Trash Wheel") %>%
  summarise(
    total_weight = 
      sum(
        weight_tons, na.rm = TRUE))
```

Calculate number of cigarette butts were collected in June 2022 by
gwynnda trash wheel. I ran into a lot of issues trying to do this by
filtering from a the gwynnda dataset directly, so I took a more long
winded approach that resulted in the total sum of cigarette butts
collected by gwynnda in June 2022.

``` r
gwynnda_cigs = 
  gwynnda_wheel_data %>%
  select(
    date, cigarette_butts)

gwynnda_cigs = 
  gwynnda_cigs %>%
  mutate(
    date = as.Date(
      date, format = "%Y-%m-%d"))

total_cigs_gwynnda = 
  gwynnda_cigs %>%
  summarise(
    total_cig_butts = 
      sum(
        cigarette_butts, na.rm = TRUE))
```

The combined dataset from the three Trash Wheels contains 1033
observations.

Professor Trash Wheel collected a total of 246.74 tons of trash.

Gwyndda Trash Wheel collected a total of 6.2392^{5} in June 2022.

## Problem 3

This problem uses data from the Great British Bake Off, a show that I
watch and love. I began by importing the `bakers` dataset. Used the
separate command to split the full names in the *baker_name* column so I
can use the first name as an ID variable across all of the datasets.
There are people who have the same first name across series, but I will
get around that by filtering by series first, and then by first name.

``` r
bakers_data = 
  read_csv("data/bakers.csv", na = c("NA", ".")) %>%  
  janitor::clean_names()

bakers_data = 
  bakers_data %>%
  separate(
    baker_name, 
    into = c("first_name", "last_name"), sep = " ")
```

Imported the `bakes` dataset. I renamed the *baker* variable to
*first_name* to standardize this variable across datasets.

``` r
bakes_data = 
  read_csv("data/bakes.csv", na = c("NA", "."))%>%  
  janitor::clean_names()

bakes_data = 
  bakes_data %>%
  rename(first_name = baker)
```

Imported the `results` dataset. I renamed the *baker* variable to
*first_name* to standardize this variable across datasets.

``` r
results_data = 
  read_csv("data/results.csv", skip = 2, na = c("NA", "."))%>%  
  janitor::clean_names()

results_data = 
  results_data %>%
  rename(
    first_name = baker)
```

Use `anti_join` to see if there are any discrepancies between the
*bakes* and *bakers* datasets. I found that the baker with first name
“Jo” in series 2 who appears in the *bakes* dataset but not the *bakers*
dataset.

``` r
missing_bakers = anti_join(bakes_data, bakers_data, by = c("series", "first_name"))
```

Using `anti_join` again, I check to see if there are any discrepancies
between the *results* dataset and the *bakers* dataset. I found that a
baker with first name *Joanne* from series 2, appears in the *results*
dataset but not the *bakers* dataset.

``` r
missing_results = 
  anti_join(
    results_data, 
    bakers_data, 
    by = c("series", "first_name"))
```

``` r
missing_bakes = 
  anti_join(
    bakes_data, 
    results_data, 
    by = c("series", "first_name"))
```

It was a little suspicious that both bakers with these discrepancies
were from series 2. I saw that in the `results` dataset, “Jo” was listed
as the winner of series 2. I decided to search online for the name of
the winner of series 2, and I found that it was Joanne Wheatley. I
looked through the *bakers* dataset, series 2 bakers, and found that the
mysterious “Jo” had the last name Wheatley. So it looks like this baker
was listed with her nickname in the *bakers* dataset and with her proper
name in the other two datasets. With confidence that these are the same
people, I will use the `mutate` and `if.else` functions to change her
name to Joanne in the *bakers* dataset before merging.

``` r
bakers_data = 
  bakers_data %>%
  mutate(
    first_name = ifelse(first_name == "Jo", "Joanne", first_name))
```

I now also have to go through a similar process with the *bake* dataset,
but her name appears as “Jo” with quotation marks. Therefore, I need to
use the additional `gsub` function to remove those quotation marks,
before I can use `mutate` to change her name to Joanne.

``` r
bakes_data = 
  bakes_data %>%
  mutate(
    first_name = gsub('"', '', first_name)) %>%
  mutate(
    first_name = ifelse(first_name == "Jo", "Joanne", first_name))
```

In looking through the separate datasets, I realized that the *bakers*
dataset only has baker information for series 5-8. So in the full
dataset, I won’t have personal information, including last name, for
bakers in series 9-10.

I can now begin to merge the datasets together using the `full_join`
function, starting with the *bakers* and *bakes* datasets. I am merging
the datasets based off the two variables they have in common, *series*
and *first_name*. I used `full_join` because I know that there is
missing data for series 9-10.

``` r
full_gbb_data = 
  bakes_data %>%
  full_join(
    bakers_data, by = c("series", "first_name"))
```

It looks like the merge was successful, so I proceed to merge the full
great British bake off dataset with the *results* dataset. I am merging
the datasets based off the three variables they have in common,
*series*, *episode*, and *first_name*.

``` r
full_gbb_data = 
  full_gbb_data %>%
  full_join(
    results_data, by = c("series", "episode", "first_name"))
```

The three datasets have been successfully merged. Now, to clean up the
full dataset a bit more. The *last_name* variable is not next to the
*first_name* variable, which makes navigating the dataset a bit
non-intuitive. I also want to move the technical challenge to appear in
the correct order next, between the columns for the other two
challenges. I can do this by using the `relocate` function. The
challenge order in the show is *signature*, *technical*, and then *show
stopper*.

``` r
full_gbb_data = 
  full_gbb_data %>%
  relocate(
    first_name, last_name, .before = series)%>%
  relocate(
    technical, .after = signature_bake)%>%
  relocate(
    result, .after = show_stopper)
```

The final dataset has the variable columns organized in a more intuitive
manner, with names at the very beginning, followed by show related
information, and personal information. The personal information for the
bakers is unfortunately appears repeated since there are multiple rows
per series with the same baker. I could not think of a way around this,
but I can’t think of a drawback for leaving those columns as such.

Next, I export the results to a csv file in my directory.

``` r
write_csv(full_gbb_data, "full_gbb_data.csv")
```

To create the reader-friendly table showing the star bakers and winners,
I create a new dataset just for the star bakers and winners from series
5 to 10.

``` r
stars_winners_data = 
  full_gbb_data %>%
  filter(
    series >= 5 & series <= 10, result == "STAR BAKER" | result == "WINNER") %>%
  select(
    series, episode, first_name, last_name, result)
```

Using the `kable` function, I can create a pretty and reader-friendly
table summarizing the start baker and winner data from the new
*starts_winners_data* dataset.

``` r
kable(stars_winners_data)
```

| series | episode | first_name | last_name     | result     |
|-------:|--------:|:-----------|:--------------|:-----------|
|      5 |       1 | Nancy      | Birtwhistle   | STAR BAKER |
|      5 |       2 | Richard    | Burr          | STAR BAKER |
|      5 |       3 | Luis       | Troyano       | STAR BAKER |
|      5 |       4 | Richard    | Burr          | STAR BAKER |
|      5 |       5 | Kate       | Henry         | STAR BAKER |
|      5 |       6 | Chetna     | Makan         | STAR BAKER |
|      5 |       7 | Richard    | Burr          | STAR BAKER |
|      5 |       8 | Richard    | Burr          | STAR BAKER |
|      5 |       9 | Richard    | Burr          | STAR BAKER |
|      5 |      10 | Nancy      | Birtwhistle   | WINNER     |
|      6 |       1 | Marie      | Campbell      | STAR BAKER |
|      6 |       2 | Ian        | Cumming       | STAR BAKER |
|      6 |       3 | Ian        | Cumming       | STAR BAKER |
|      6 |       4 | Ian        | Cumming       | STAR BAKER |
|      6 |       5 | Nadiya     | Hussain       | STAR BAKER |
|      6 |       6 | Mat        | Riley         | STAR BAKER |
|      6 |       7 | Tamal      | Ray           | STAR BAKER |
|      6 |       8 | Nadiya     | Hussain       | STAR BAKER |
|      6 |       9 | Nadiya     | Hussain       | STAR BAKER |
|      6 |      10 | Nadiya     | Hussain       | WINNER     |
|      7 |       1 | Jane       | Beedle        | STAR BAKER |
|      7 |       2 | Candice    | Brown         | STAR BAKER |
|      7 |       3 | Tom        | Gilliford     | STAR BAKER |
|      7 |       4 | Benjamina  | Ebuehi        | STAR BAKER |
|      7 |       5 | Candice    | Brown         | STAR BAKER |
|      7 |       6 | Tom        | Gilliford     | STAR BAKER |
|      7 |       7 | Andrew     | Smyth         | STAR BAKER |
|      7 |       8 | Candice    | Brown         | STAR BAKER |
|      7 |       9 | Andrew     | Smyth         | STAR BAKER |
|      7 |      10 | Candice    | Brown         | WINNER     |
|      8 |       1 | Steven     | Carter-Bailey | STAR BAKER |
|      8 |       2 | Steven     | Carter-Bailey | STAR BAKER |
|      8 |       3 | Julia      | Chernogorova  | STAR BAKER |
|      8 |       4 | Kate       | Lyon          | STAR BAKER |
|      8 |       5 | Sophie     | Faldo         | STAR BAKER |
|      8 |       6 | Liam       | Charles       | STAR BAKER |
|      8 |       7 | Steven     | Carter-Bailey | STAR BAKER |
|      8 |       8 | Stacey     | Hart          | STAR BAKER |
|      8 |       9 | Sophie     | Faldo         | STAR BAKER |
|      8 |      10 | Sophie     | Faldo         | WINNER     |
|      9 |       1 | Manon      | NA            | STAR BAKER |
|      9 |       2 | Rahul      | NA            | STAR BAKER |
|      9 |       3 | Rahul      | NA            | STAR BAKER |
|      9 |       4 | Dan        | NA            | STAR BAKER |
|      9 |       5 | Kim-Joy    | NA            | STAR BAKER |
|      9 |       6 | Briony     | NA            | STAR BAKER |
|      9 |       7 | Kim-Joy    | NA            | STAR BAKER |
|      9 |       8 | Ruby       | NA            | STAR BAKER |
|      9 |       9 | Ruby       | NA            | STAR BAKER |
|      9 |      10 | Rahul      | NA            | WINNER     |
|     10 |       1 | Michelle   | NA            | STAR BAKER |
|     10 |       2 | Alice      | NA            | STAR BAKER |
|     10 |       3 | Michael    | NA            | STAR BAKER |
|     10 |       4 | Steph      | NA            | STAR BAKER |
|     10 |       5 | Steph      | NA            | STAR BAKER |
|     10 |       6 | Steph      | NA            | STAR BAKER |
|     10 |       7 | Henry      | NA            | STAR BAKER |
|     10 |       8 | Steph      | NA            | STAR BAKER |
|     10 |       9 | Alice      | NA            | STAR BAKER |
|     10 |      10 | David      | NA            | WINNER     |

Based on the table, there were some predictable and unpredictable
winners. From series 5, Nancy Birtwhistle was the winner, but she had
only won star baker once, whereas Richard Burr had won star baker 5/10
episodes. That was a surprise. In series 6, the win could have really
gone either way between Ian Cumming and the winner Nadiya Hussein, as
they each won 3/10 star bakers. In series 7, Candice Brown was the
predictable winner with 4/10 star bakers. In series 8, the winner was
not predictable as it could have gone either way between Steven Carter-
Bailey and winner Sophie Faldo, who each won 3/10 star bakers. In series
9 and 10, I don’t have last names. In series 9, Rahul won and certainly
had the most collective star baker wins out of everyone in the season
with 3/10. However, in series 10, it really looked like Steph was going
to win, with 4/10 star baker wins, but the winner was David, who had not
won ANY of the star bakers throughout the season.

From this information, if looks like most winners have around 3 star
baker wins throughout the season.

Finally, I imported the *viewers* dataset.

``` r
viewers_data = 
  read_csv(
    "data/viewers.csv", na = c("NA", ".")) %>% 
  janitor::clean_names()
```

View the first 10 rows of this dataset using the `head` function.

``` r
head(viewers_data, 10)
```

    ## # A tibble: 10 × 11
    ##    episode series_1 series_2 series_3 series_4 series_5 series_6 series_7
    ##      <dbl>    <dbl>    <dbl>    <dbl>    <dbl>    <dbl>    <dbl>    <dbl>
    ##  1       1     2.24     3.1      3.85     6.6      8.51     11.6     13.6
    ##  2       2     3        3.53     4.6      6.65     8.79     11.6     13.4
    ##  3       3     3        3.82     4.53     7.17     9.28     12.0     13.0
    ##  4       4     2.6      3.6      4.71     6.82    10.2      12.4     13.3
    ##  5       5     3.03     3.83     4.61     6.95     9.95     12.4     13.1
    ##  6       6     2.75     4.25     4.82     7.32    10.1      12       13.1
    ##  7       7    NA        4.42     5.1      7.76    10.3      12.4     13.4
    ##  8       8    NA        5.06     5.35     7.41     9.02     11.1     13.3
    ##  9       9    NA       NA        5.7      7.41    10.7      12.6     13.4
    ## 10      10    NA       NA        6.74     9.45    13.5      15.0     15.9
    ## # ℹ 3 more variables: series_8 <dbl>, series_9 <dbl>, series_10 <dbl>

Next, I calculated the average viewership in season 1.

``` r
s1_avg_viewers = 
  viewers_data %>%
   summarise(
     avg_viewership = mean(series_1, na.rm = TRUE))
```

The average viewership in season 1 was 2.77, with a unit I assume to be
million.

Finally, I calculated the average viewership in season 5.

``` r
s5_avg_viewers = 
  viewers_data %>%
   summarise(
     avg_viewership = mean(series_5, na.rm = TRUE))
```

The average viewership in season 1 was 10.0393, with a unit I assume to
be million.
