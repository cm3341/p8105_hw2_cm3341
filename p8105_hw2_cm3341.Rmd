---
title: "p8105_hw2_cm3341"
author: "Carolina Montes Garcia"
date: "2024-10-01"
output: 
  github_document:
    toc: true
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE)

```

### Problem 0

This solution focuses on a reproducible report containing code and text necessary for Problems 1-3, and is organized as an R Project. This was not prepared as a GitHub repo; examples for repository structure and git commits should be familiar from other elements of the course.

Throughout, we use appropriate text to describe our code and results, and use clear styling to ensure code is readable. 


Load libraries

```{r}
library(tidyverse)
library(readxl)
```

## Problem 1

Below we import and clean data from `NYC_Transit_Subway_Entrance_And_Exit_Data.csv`. The process begins with data import, updates variable names, and selects the columns that will be used in later parts fo this problem. We update `entry` from `yes` / `no` to a logical variable. As part of data import, we specify that `Route` columns 8-11 should be character for consistency with 1-7.

```{r}
trans_ent = 
  read_csv(
    "data/NYC_Transit_Subway_Entrance_And_Exit_Data.csv",
    col_types = cols(Route8 = "c", Route9 = "c", Route10 = "c", Route11 = "c")) |> 
  janitor::clean_names() |> 
  select(
    line, station_name, station_latitude, station_longitude, 
    starts_with("route"), entry, exit_only, vending, entrance_type, 
    ada) |> 
  mutate(entry = ifelse(entry == "YES", TRUE, FALSE))
```

As it stands, these data are not "tidy": route number should be a variable, as should route. That is, to obtain a tidy dataset we would need to convert `route` variables from wide to long format. This will be useful when focusing on specific routes, but may not be necessary when considering questions that focus on station-level variables. 

The following code chunk selects station name and line, and then uses `distinct()` to obtain all unique combinations. As a result, the number of rows in this dataset is the number of unique stations. .

```{r}
trans_ent |> 
  select(station_name, line) |> 
  distinct()
```

The next code chunk is similar, but filters according to ADA compliance as an initial step. This produces a dataframe in which the number of rows is the number of ADA compliant stations. 

```{r}
trans_ent |> 
  filter(ada == TRUE) |> 
  select(station_name, line) |> 
  distinct()
```

To compute the proportion of station entrances / exits without vending allow entrance, we first exclude station entrances that do not allow vending. Then, we focus on the `entry` variable -- this logical, so taking the mean will produce the desired proportion (recall that R will coerce logical to numeric in cases like this).

```{r}
trans_ent |> 
  filter(vending == "NO") |> 
  pull(entry) |> 
  mean()
```

Lastly, we write a code chunk to identify stations that serve the A train, and to assess how many of these are ADA compliant. As a first step, we tidy the data as alluded to previously; that is, we convert `route` from wide to long format. After this step, we can use tools from previous parts of the question (filtering to focus on the A train, and on ADA compliance; selecting and using `distinct` to obtain dataframes with the required stations in rows).

```{r}
trans_ent |> 
  pivot_longer(
    route1:route11,
    names_to = "route_num",
    values_to = "route") |> 
  filter(route == "A") |> 
  select(station_name, line) |> 
  distinct()

trans_ent |> 
  pivot_longer(
    route1:route11,
    names_to = "route_num",
    values_to = "route") |> 
  filter(route == "A", ada == TRUE) |> 
  select(station_name, line) |> 
  distinct()
```

## Problem 2

This problem uses trash wheel datasets for three different trash wheel garbage collector vehicles (Mr. Trash Wheel, Professor Trash Wheel, and Gwynnda Trash Wheel). These machines clean debris from various harbors. 
First, I imported and cleaned the Mr. trash wheel dataset. The *year* variable has to be mutated into the same type of variable so that the binding function can work the dataset merging step later on. I chose to make all years into numeric variables using `as.numeric`.
```{r}

mr_trash_wheel_data <- read_excel("data/202409 Trash Wheel Collection Data.xlsx", sheet = "Mr. Trash Wheel", skip = 1, na = c("NA", ".")) %>%
  janitor::clean_names() %>%
  slice(1:(n() - 2)) %>%
  mutate(sports_balls = as.integer(round(sports_balls, 0)),  
         trash_wheel = "Mr. Trash Wheel")%>%
  select(-x15, -x16)%>%
  mutate(year = as.numeric(year))
```


Import and clean professor trash wheel data. 

```{r}

prof_wheel_data <- read_excel("data/202409 Trash Wheel Collection Data.xlsx", sheet = "Professor Trash Wheel", skip = 1, na = c("NA", ".")) %>%
  janitor::clean_names() %>%
  slice(1:(n() - 2)) %>%
  mutate(trash_wheel = "Professor Trash Wheel")%>%
  mutate(year = as.numeric(year))
  
```

Import and clean gwynnda trash wheel data.
```{r}
gwynnda_wheel_data <- read_excel("data/202409 Trash Wheel Collection Data.xlsx", sheet = "Gwynnda Trash Wheel", skip = 1, na = c("NA", ".")) %>%
  janitor::clean_names() %>%
  slice(1:(n() - 1)) %>%
  mutate(trash_wheel = "Gwynnda Trash Wheel")%>%
  mutate(year = as.numeric(year))
```

Combine the three datasets. 

```{r}

full_trash_wheel_data = bind_rows(mr_trash_wheel_data, prof_wheel_data, gwynnda_wheel_data)
```

Calculate # of total observations in the combined dataset
```{r}
total_rows <- nrow(full_trash_wheel_data)

```

Calculate how many tons of trash were collected by professor trash wheel.
```{r}

prof_trash_weight = full_trash_wheel_data %>%
  filter(trash_wheel == "Professor Trash Wheel") %>%
  summarise(total_weight = sum(weight_tons, na.rm = TRUE))
```


```{r}
gwynnda_cigs_062022 = full_trash_wheel_data %>%
  filter(trash_wheel == "Gwynnda", format(as.Date(date), "%Y-%m") == "2022-06") %>%
  summarise(total_cig_butts = sum(cigarette_butts, na.rm = TRUE))

```

The combined dataset from the three Trash Wheels contains `r total_rows` observations. 

Professor Trash Wheel collected a total of `r prof_trash_weight` tons of trash. 

- Issue with Gwynnda cigs, check code-

## Problem 3

Import the `bakers` dataset. Used the separate command to split the full names in the *baker_name* column so I can use the first name as an ID variable across all of the datasets. There are people who have the same first name across series, but I will get around that by filtering by series first, and then by first name. 

```{r}

bakers_data = 
  read_csv("data/bakers.csv", na = c("NA", ".")) %>%  
  janitor::clean_names()

bakers_data = bakers_data %>%
  separate(baker_name, into = c("first_name", "last_name"), sep = " ")

```

Imported the `bakes` dataset. I renamed the *baker* variable to *first_name* to standardize this variable across datasets. 

```{r}
bakes_data = 
  read_csv("data/bakes.csv", na = c("NA", "."))%>%  
  janitor::clean_names()

bakes_data = 
  bakes_data %>%
  rename(first_name = baker)

```

Imported the `results` dataset. I renamed the *baker* variable to *first_name* to standardize this variable across datasets. 
```{r}
results_data = 
  read_csv("data/results.csv", skip = 2, na = c("NA", "."))%>%  
  janitor::clean_names()

results_data = 
  results_data %>%
  rename(first_name = baker)
```

Use `anti_join` to see if there are any discrepancies between the *bakes* and *bakers* datasets. I found that the baker with first name "Jo" in series 2 who appears in the *bakes* dataset but not the *bakers* dataset. 

```{r}
missing_bakers = anti_join(bakes_data, bakers_data, by = c("series", "first_name"))
```

Using `anti_join` again, I check to see if there are any discrepancies between the *results* dataset and the *bakers* dataset. I found that a baker with first name *Joanne* from series 2, appears in the *results* dataset but not the *bakers* dataset. 

```{r}
missing_results = anti_join(results_data, bakers_data, by = c("series", "first_name"))
```

For now, I don't see a reason to eliminate the rows for the bakers I found through `anti_join`. They are both from series 2 and the summarizing I will be doing in the next section of the problem does not involve series 2. 

I begin to merge the datasets together using the `left_join` function, starting with the *bakers* and *bakes* datasets. I am merging the datasets based off the two variables they have in common, *series* and *first_name*.  
```{r}
full_gbb_data = bakes_data %>%
  left_join(bakers_data, by = c("series", "first_name"))
```

It looks like the merge was successful, so I proceed to merge the full great British bake off dataset with the *results* dataset. I am merging the datasets based off the three variables they have in common, *series*, *episode*, and *first_name*.

```{r}
full_gbb_data = full_gbb_data %>%
  left_join(results_data, by = c("series", "episode", "first_name"))
```

The three datasets have been successfully merged. Now, to clean up the full dataset a bit more. The *last_name* variable is not next to the *first_name* variable, which makes navigating the dataset a bit non-intuitive. I also want to move the technical challenge to appear in the correct order next, between the columns for the other two challenges. I can do this by using the `relocate` function. The challenge order in the show is *signature*, *technical*, and then *show stopper*. 

```{r}
full_gbb_data = full_gbb_data %>%
  relocate(first_name, last_name, .before = series)%>%
  relocate(technical, .after = signature_bake)
```






```{r}
write_csv(full_gbb_data, "full_gbb_data.csv")
```

