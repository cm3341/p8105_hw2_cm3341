---
title: "p8105_hw2_cm3341"
author: "Carolina Montes Garcia"
date: "2024-10-01"
output: 
  github_document:
    toc: true
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE)

```

### Problem 0

This solution focuses on a reproducible report containing code and text necessary for Problems 1-3, and is organized as an R Project. This was not prepared as a GitHub repo; examples for repository structure and git commits should be familiar from other elements of the course.

Throughout, we use appropriate text to describe our code and results, and use clear styling to ensure code is readable. 


Load libraries

```{r}
library(tidyverse)
library(readxl)
library(knitr)
```

## Problem 1

Below we import and clean data from `NYC_Transit_Subway_Entrance_And_Exit_Data.csv`. The process begins with data import, updates variable names, and selects the columns that will be used in later parts fo this problem. We update `entry` from `yes` / `no` to a logical variable. As part of data import, we specify that `Route` columns 8-11 should be character for consistency with 1-7.

```{r}
trans_ent = 
  read_csv(
    "data/NYC_Transit_Subway_Entrance_And_Exit_Data.csv",
    col_types = cols(Route8 = "c", Route9 = "c", Route10 = "c", Route11 = "c")) |> 
  janitor::clean_names() |> 
  select(
    line, station_name, station_latitude, station_longitude, 
    starts_with("route"), entry, exit_only, vending, entrance_type, 
    ada) |> 
  mutate(entry = ifelse(entry == "YES", TRUE, FALSE))
```

As it stands, these data are not "tidy": route number should be a variable, as should route. That is, to obtain a tidy dataset we would need to convert `route` variables from wide to long format. This will be useful when focusing on specific routes, but may not be necessary when considering questions that focus on station-level variables. 

The following code chunk selects station name and line, and then uses `distinct()` to obtain all unique combinations. As a result, the number of rows in this dataset is the number of unique stations. .

```{r}
trans_ent |> 
  select(station_name, line) |> 
  distinct()
```

The next code chunk is similar, but filters according to ADA compliance as an initial step. This produces a dataframe in which the number of rows is the number of ADA compliant stations. 

```{r}
trans_ent |> 
  filter(ada == TRUE) |> 
  select(station_name, line) |> 
  distinct()
```

To compute the proportion of station entrances / exits without vending allow entrance, we first exclude station entrances that do not allow vending. Then, we focus on the `entry` variable -- this logical, so taking the mean will produce the desired proportion (recall that R will coerce logical to numeric in cases like this).

```{r}
trans_ent |> 
  filter(vending == "NO") |> 
  pull(entry) |> 
  mean()
```

Lastly, we write a code chunk to identify stations that serve the A train, and to assess how many of these are ADA compliant. As a first step, we tidy the data as alluded to previously; that is, we convert `route` from wide to long format. After this step, we can use tools from previous parts of the question (filtering to focus on the A train, and on ADA compliance; selecting and using `distinct` to obtain dataframes with the required stations in rows).

```{r}
trans_ent |> 
  pivot_longer(
    route1:route11,
    names_to = "route_num",
    values_to = "route") |> 
  filter(route == "A") |> 
  select(station_name, line) |> 
  distinct()

trans_ent |> 
  pivot_longer(
    route1:route11,
    names_to = "route_num",
    values_to = "route") |> 
  filter(route == "A", ada == TRUE) |> 
  select(station_name, line) |> 
  distinct()
```

## Problem 2

This problem uses trash wheel datasets for three different trash wheel garbage collector vehicles (Mr. Trash Wheel, Professor Trash Wheel, and Gwynnda Trash Wheel). These machines clean debris from various harbors. 
First, I imported and cleaned the Mr. trash wheel dataset. The *year* variable has to be mutated into the same type of variable so that the binding function can work the dataset merging step later on. I chose to make all years into numeric variables using `as.numeric`. 

In my import, I skipped the first row as it contained unimportant information. Additionally, I used `slice` to remove the last two rows of the dataset that included some calculations. I then removed two more random columns that appear after import called *x15* and *x16*, which contained no information at all. 
```{r}

mr_trash_wheel_data <- read_excel("data/202409 Trash Wheel Collection Data.xlsx", sheet = "Mr. Trash Wheel", skip = 1, na = c("NA", ".")) %>%
  janitor::clean_names() %>%
  slice(1:(n() - 2)) %>%
  mutate(sports_balls = as.integer(round(sports_balls, 0)),  
         trash_wheel = "Mr. Trash Wheel")%>%
  select(-x15, -x16)%>%
  mutate(year = as.numeric(year))
```


Import and clean professor trash wheel data. In my import, I skipped the first row as it contained unimportant information. Additionally, I used `slice` to remove the last two rows of the dataset that included some calculations.

```{r}

prof_wheel_data <- read_excel("data/202409 Trash Wheel Collection Data.xlsx", sheet = "Professor Trash Wheel", skip = 1, na = c("NA", ".")) %>%
  janitor::clean_names() %>%
  slice(1:(n() - 2)) %>%
  mutate(trash_wheel = "Professor Trash Wheel")%>%
  mutate(year = as.numeric(year))
  
```

Import and clean gwynnda trash wheel data.In my import, I skipped the first row as it contained unimportant information. Additionally, I used `slice` to remove the last row of the dataset that included some calculations.

```{r}
gwynnda_wheel_data <- read_excel("data/202409 Trash Wheel Collection Data.xlsx", sheet = "Gwynnda Trash Wheel", skip = 1, na = c("NA", ".")) %>%
  janitor::clean_names() %>%
  slice(1:(n() - 1)) %>%
  mutate(trash_wheel = "Gwynnda Trash Wheel")%>%
  mutate(year = as.numeric(year))
```

I combined the three datasets using the `bind_rows` function, since I need these rows to stack on top of each other. I tried merge functions first, but that didn't work. 

```{r}

full_trash_wheel_data = bind_rows(mr_trash_wheel_data, prof_wheel_data, gwynnda_wheel_data)
```

Calculated # of total observations in the combined dataset
```{r}
total_rows <- nrow(full_trash_wheel_data)

```

Calculated how many tons of trash were collected by professor trash wheel.
```{r}

prof_trash_weight = full_trash_wheel_data %>%
  filter(trash_wheel == "Professor Trash Wheel") %>%
  summarise(total_weight = sum(weight_tons, na.rm = TRUE))
```

Calculate number of cigarette butts were collected in June 2022 by gwynnda trash wheel. I ran into a lot of issues trying to do this by filtering from a the gwynnda dataset directly, so I took a more long winded approach that resulted in the total sum of cigarette butts collected by gwynnda in June 2022. 

```{r}

gwynnda_cigs = gwynnda_wheel_data %>%
  select(date, cigarette_butts)

gwynnda_cigs = gwynnda_cigs %>%
  mutate(date = as.Date(date, format = "%Y-%m-%d"))

total_cigs_gwynnda = gwynnda_cigs %>%
  summarise(total_cig_butts = sum(cigarette_butts, na.rm = TRUE))

```

The combined dataset from the three Trash Wheels contains `r total_rows` observations. 

Professor Trash Wheel collected a total of `r prof_trash_weight` tons of trash. 

Gwyndda Trash Wheel collected a total of `r total_cigs_gwynnda` in June 2022. 

## Problem 3

Import the `bakers` dataset. Used the separate command to split the full names in the *baker_name* column so I can use the first name as an ID variable across all of the datasets. There are people who have the same first name across series, but I will get around that by filtering by series first, and then by first name. 

```{r}

bakers_data = 
  read_csv("data/bakers.csv", na = c("NA", ".")) %>%  
  janitor::clean_names()

bakers_data = bakers_data %>%
  separate(baker_name, into = c("first_name", "last_name"), sep = " ")

```

Imported the `bakes` dataset. I renamed the *baker* variable to *first_name* to standardize this variable across datasets. 

```{r}
bakes_data = 
  read_csv("data/bakes.csv", na = c("NA", "."))%>%  
  janitor::clean_names()

bakes_data = 
  bakes_data %>%
  rename(first_name = baker)

```

Imported the `results` dataset. I renamed the *baker* variable to *first_name* to standardize this variable across datasets. 
```{r}
results_data = 
  read_csv("data/results.csv", skip = 2, na = c("NA", "."))%>%  
  janitor::clean_names()

results_data = 
  results_data %>%
  rename(first_name = baker)
```

Use `anti_join` to see if there are any discrepancies between the *bakes* and *bakers* datasets. I found that the baker with first name "Jo" in series 2 who appears in the *bakes* dataset but not the *bakers* dataset. 

```{r}
missing_bakers = anti_join(bakes_data, bakers_data, by = c("series", "first_name"))
```

Using `anti_join` again, I check to see if there are any discrepancies between the *results* dataset and the *bakers* dataset. I found that a baker with first name *Joanne* from series 2, appears in the *results* dataset but not the *bakers* dataset. 

```{r}
missing_results = anti_join(results_data, bakers_data, by = c("series", "first_name"))
```

```{r}
missing_bakes = anti_join(bakes_data, results_data, by = c("series", "first_name"))
```


It was a little suspicious that both bakers with these discrepancies were from series 2. I saw that in the `results` dataset, "Jo" was listed as the winner of series 2. I decided to search online for the name of the winner of series 2, and I found that it was Joanne Wheatley. I looked through the *bakers* dataset, series 2 bakers, and found that the mysterious "Jo" had the last name Wheatley. So it looks like this baker was listed with her nickname in the *bakers* dataset and with her proper name in the other two datasets. With confidence that these are the same people, I will use the `mutate` and `if.else` functions to change her name to Joanne in the *bakers* dataset before merging. 

```{r}
bakers_data = bakers_data %>%
  mutate(first_name = ifelse(first_name == "Jo", "Joanne", first_name))

```

I now also have to go through a similar process with the *bake* dataset, but her name appears as "Jo" with quotation marks. Therefore, I need to use the additional `gsub` function to remove those quotation marks, before I can use `mutate` to change her name to Joanne. 

```{r}
bakes_data <- bakes_data %>%
  mutate(first_name = gsub('"', '', first_name)) %>%
  mutate(first_name = ifelse(first_name == "Jo", "Joanne", first_name))
```

In looking through the separate datasets, I realized that the *bakers* dataset only has baker information for series 5-8. So in the full dataset, I won't have personal information, including last name, for bakers in series 9-10. 

I can now begin to merge the datasets together using the `full_join` function, starting with the *bakers* and *bakes* datasets. I am merging the datasets based off the two variables they have in common, *series* and *first_name*. I used `full_join` because I know that there is missing data for series 9-10. 

```{r}
full_gbb_data = bakes_data %>%
  full_join(bakers_data, by = c("series", "first_name"))
```

It looks like the merge was successful, so I proceed to merge the full great British bake off dataset with the *results* dataset. I am merging the datasets based off the three variables they have in common, *series*, *episode*, and *first_name*.

```{r}
full_gbb_data = full_gbb_data %>%
  full_join(results_data, by = c("series", "episode", "first_name"))
```

The three datasets have been successfully merged. Now, to clean up the full dataset a bit more. The *last_name* variable is not next to the *first_name* variable, which makes navigating the dataset a bit non-intuitive. I also want to move the technical challenge to appear in the correct order next, between the columns for the other two challenges. I can do this by using the `relocate` function. The challenge order in the show is *signature*, *technical*, and then *show stopper*. 

```{r}
full_gbb_data = full_gbb_data %>%
  relocate(first_name, last_name, .before = series)%>%
  relocate(technical, .after = signature_bake)%>%
  relocate(result, .after = show_stopper)
```

The final dataset has the variable columns organized in a more intuitive manner, with names at the very beginning, followed by show related information, and personal information. The personal information for the bakers is unfortunately appears repeated since there are multiple rows per series with the same baker. I could not think of a way around this, but I can't think of a drawback for leaving those columns as such. 

Next, I export the results to a csv file in my directory.
```{r}
write_csv(full_gbb_data, "full_gbb_data.csv")
```

To create the reader-friendly table showing the star bakers and winners, I create a new dataset just for the star bakers and winners from series 5 to 10. 
```{r}
stars_winners_data = 
  full_gbb_data %>%
  filter(
    series >= 5 & series <= 10, result == "STAR BAKER" | result == "WINNER") %>%
  select(
    series, episode, first_name, last_name, result)
```

Using the `kable` function, I can create a pretty and reader-friendly table summarizing the start baker and winner data from the new *starts_winners_data* dataset. 
```{r}
kable(stars_winners_data)
```

Based on the table, there were some predictable and unpredictable winners. From series 5, Nancy Birtwhistle was the winner, but she had only won star baker once, whereas Richard Burr had won star baker 5/10 episodes. That was a surprise. In series 6, the win could have really gone either way between Ian Cumming and the winner Nadiya Hussein, as they each won 3/10 star bakers. In series 7, Candice Brown was the predictable winner with 4/10 star bakers. In series 8, the winner was not predictable as it could have gone either way between Steven Carter- Bailey and winner Sophie Faldo, who each won 3/10 star bakers. In series 9 and 10, I don't have last names. In series 9, Rahul won and certainly had the most collective star baker wins out of everyone in the season with 3/10. However, in series 10, it really looked like Steph was going to win, with 4/10 star baker wins, but the winner was David, who had not won ANY of the star bakers throughout the season.

From this information, if looks like most winners have around 3 star baker wins throughout the season. 


Finally, I imported the *viewers* dataset. 
```{r}
viewers_data = 
  read_csv(
    "data/viewers.csv", na = c("NA", ".")) %>% 
  janitor::clean_names()
```

View the first 10 rows of this dataset using the `head` function.

```{r}
head(viewers_data, 10)
```

Next, I calculated the average viewership in season 1.

```{r}
s1_avg_viewers = 
  viewers_data %>%
   summarise(avg_viewership = mean(series_1, na.rm = TRUE))
```

The average viewership in season 1 was `r s1_avg_viewers`, with a unit I assume to be million. 

Finally, I calculated the average viewership in season 5.

```{r}
s5_avg_viewers = 
  viewers_data %>%
   summarise(avg_viewership = mean(series_5, na.rm = TRUE))
```

The average viewership in season 1 was `r s5_avg_viewers`, with a unit I assume to be million. 
